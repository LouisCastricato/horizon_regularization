{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "#from data_twitter import *\n",
    "#from gnn_twitter import GNN_Twitter\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "#from tqdm import tqdm\n",
    "import statistics\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob, os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "import operator\n",
    "import json\n",
    "\n",
    "from bert_embedding import BertEmbedding\n",
    "import mxnet as mx\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish constructing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def load(file, direct = 'pre/opinosis/'):\n",
    "    arr = []\n",
    "    with open(direct + file) as opin:\n",
    "        for line in opin:\n",
    "            arr.append(line)\n",
    "    return(arr)\n",
    "def dirload(file, direct = 'pre/opinosis/'):\n",
    "    arr = \"\"\n",
    "    with open(direct + file) as opin:\n",
    "        for line in opin:\n",
    "            arr += line\n",
    "    if arr == \"\":\n",
    "        return None\n",
    "    else:\n",
    "        return arr\n",
    "\n",
    "def fixdoubleindent(arr):      \n",
    "    narr = np.array(arr)\n",
    "    narr = narr[np.where(narr != '\\n')]\n",
    "    return narr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggressive_fix_punc(string):\n",
    "    string = string.replace('\\n','')\n",
    "    string = string.replace('.', '')\n",
    "    string = string.replace('!', '')\n",
    "    string = string.replace('?', '')\n",
    "    string = string.replace(',', '')\n",
    "    return(string)    \n",
    "def fix_punc(string):\n",
    "    string = string.replace('\\n','')\n",
    "    string = string.replace(' . ', '.')\n",
    "    string = string.replace(' !', '!')\n",
    "    string = string.replace(' ?', '?')\n",
    "    string = string.replace(' ,', ',')\n",
    "\n",
    "    return(string)\n",
    "def fix_punc2(string):\n",
    "    string = string.replace('\\n','')\n",
    "    string = string.replace(' .', '.')\n",
    "    string = string.replace(' !', '!')\n",
    "    string = string.replace(' ?', '?')\n",
    "    string = string.replace(' ,', ',')\n",
    "    return(string)\n",
    "def narr_fix_punc(narr):\n",
    "    for i in range(len(narr)):\n",
    "        narr[i] = fix_punc(narr[i])\n",
    "    return(narr)\n",
    "def anarr_fix_punc(narr):\n",
    "    for i in range(len(narr)):\n",
    "        narr[i] = aggressive_fix_punc(narr[i])\n",
    "    return(narr)\n",
    "def fix_jumps(narr):\n",
    "    toret = []\n",
    "    for string in narr:\n",
    "\n",
    "        string = string.split('.')\n",
    "        #if isinstance(string, list):\n",
    "        #    string = flatten(string)\n",
    "        toret.append(string)\n",
    "\n",
    "\n",
    "    return flatten(toret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin constructing the edges for our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def computesim(nparr):\n",
    "    vect = TfidfVectorizer(min_df=1)\n",
    "    tfidf = vect.fit_transform(nparr)\n",
    "\n",
    "    sim_matrix = (tfidf * tfidf.T).A\n",
    "    return(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude everything along the diagonal (and duplicates)\n",
    "def simadj(sim_mat, thres = 1.5):\n",
    "    nlen = sim_mat.shape[0]\n",
    "    df = sim_mat[np.where(sim_mat != 1)]\n",
    "\n",
    "    sd = statistics.stdev(df.flatten())\n",
    "    mu = statistics.mean(df.flatten())\n",
    "\n",
    "    out_matrx = np.zeros((nlen, nlen))\n",
    "    #Could not get np.where working for this, so I did it manually\n",
    "    #The threshold for determining if an element is an outlier\n",
    "    for i in range(nlen):\n",
    "        for j in range(nlen):\n",
    "            if i != j:\n",
    "                if sim_mat[i,j] > mu + thres * sd:\n",
    "                    out_matrx[i,j] = 1\n",
    "            #No need for <, as we only care about points on the other end of the extreme spectrum\n",
    "    return(out_matrx)\n",
    "#out_matrx is our adj matrix. I specifically avoided connecting the last sentence of a document\n",
    "#to the first sentence of all others on this data set, since each documnt is only once sentence\n",
    "\n",
    "#Actually I take the latter half of the above back, I figured out how the dataset is formatting different\n",
    "#sentences of the same article. If the sentence does not begin with a space, then then it is from\n",
    "#the same article\n",
    "#print(np.where(out_matrx== 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_punc_fix(string):\n",
    "    return string \n",
    "    '''\n",
    "    puncs = ['.','?','!']\n",
    "    \n",
    "    #Easiest case first\n",
    "    if string[-1] not in puncs:\n",
    "        string = string +' . '\n",
    "    out = ''\n",
    "    #Tokenize the text. If the first letter of a word is capital, insert a period before it\n",
    "    for s in string.split():\n",
    "        if s[0].isupper():\n",
    "            out = out + '. ' + s + ' '\n",
    "        else:\n",
    "            out = out + s + ' '\n",
    "    string = out[1:len(out)]\n",
    "    return(string)\n",
    "    '''\n",
    "def lasttofirst_str(nparr):\n",
    "    paragraphs = []\n",
    "    string = ''\n",
    "    for i in range(len(nparr)):\n",
    "        string = string + nparr[i]\n",
    "        if not nparr[i].startswith(' '):\n",
    "            strs = advanced_punc_fix(string).split()\n",
    "            for j in range(len(strs)):\n",
    "                strs[j] = strs[j].split('\\'')\n",
    "            strs = flatten(strs)\n",
    "            for j in range(len(strs)):\n",
    "                strs[j] = strs[j].split('-')\n",
    "            strs = flatten(strs)\n",
    "            paragraphs.append(strs)\n",
    "            string = ''\n",
    "    return paragraphs\n",
    "def lasttofirst_indx(nparr):\n",
    "    paragraphs = [list()]\n",
    "    j = 0\n",
    "    for i in range(len(nparr)):\n",
    "        if not nparr[i].startswith(' '):\n",
    "            try:\n",
    "                paragraphs[j].append(i)\n",
    "            except:\n",
    "                paragraphs.append(list())\n",
    "                print(len(paragraphs))\n",
    "                paragraphs[j].append(i)\n",
    "        else:\n",
    "            j += 1\n",
    "    return paragraphs\n",
    "\n",
    "def lasttofirst(indx, out,nparr):\n",
    "    \n",
    "    paragraphs = indx\n",
    "    #Make sure that the last sentence of our document points to \n",
    "    #the first sentence of every other document except\n",
    "    #itself\n",
    "    for i in paragraphs:\n",
    "        for j in paragraphs:\n",
    "            if i != j:\n",
    "                #j is the first sentence and the last sentence \n",
    "                if(j - 1 in paragraphs or j == 0 or (j == len(nparr) - 1)):\n",
    "                    out[i,j] = 1\n",
    "                else:\n",
    "                    out[i,j+1] = 1\n",
    "    return(out)\n",
    "\n",
    "def load_from_JSON(direct):\n",
    "    full_stops = ['.', ' . ', '. ', ' .',\n",
    "                  '?', ' ? ', '? ', ' ?',\n",
    "                  '!', ' ! ', '! ', ' !']\n",
    "    articles = []\n",
    "    words = list()\n",
    "    words.append(list())\n",
    "    paragraphs = list()\n",
    "    with open(direct) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        obj = data['articles']\n",
    "        sent_i = 0\n",
    "        for i in range(len(obj)):\n",
    "            articles.append([])\n",
    "            for word in obj[i]:\n",
    "                articles[i].append(word)\n",
    "                if \".\" == word or \".\\n\" == word:\n",
    "                    sent_i += 1\n",
    "        #Compile everything into a set of sentences\n",
    "        sent_count = sent_i\n",
    "        num_articles = len(obj)\n",
    "        sent_num = 0\n",
    "        word_num = 0\n",
    "        \n",
    "        for i in range(num_articles):\n",
    "            for j in range(len(articles[i])):\n",
    "                try:\n",
    "                    words[sent_num].append(articles[i][j])                    \n",
    "                except:\n",
    "                    words.append(list())\n",
    "                    words[sent_num].append(articles[i][j])\n",
    "                word_num += 1\n",
    "                #Next sentence\n",
    "                if articles[i][j] in full_stops:\n",
    "                    sent_num += 1\n",
    "                    word_num = 0\n",
    "                if sent_num >= sent_count:\n",
    "                    break\n",
    "            if sent_num >= sent_count:\n",
    "                sent_num = 0\n",
    "                break\n",
    "            sent_num += 1\n",
    "            word_num = 0\n",
    "            words.append(list())\n",
    "            paragraphs.append(sent_num)\n",
    "    return words, paragraphs\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can combine everything into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-6d9dc5e7feda>, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-6d9dc5e7feda>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    %%capture output\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "def computeadjmatrices(path):\n",
    "    adjmats = []\n",
    "    files = os.listdir(path)\n",
    "    max_size = 0\n",
    "   \n",
    "    for file in tqdm_notebook(files):\n",
    "        try:\n",
    "            mFile = Path(\"../sum_data/opinosis_graphs/\" + file.replace('.txt','') + \".csv\")\n",
    "            #if mFile.is_file():\n",
    "            #    continue\n",
    "            words, paragraphs = load_from_JSON(path + file)\n",
    "            forprocessing = list(map(lambda x: \" \".join(x), words))\n",
    "\n",
    "            out = simadj(computesim(forprocessing), thres = 1.2)\n",
    "\n",
    "            out = lasttofirst(paragraphs, out, forprocessing)\n",
    "            size = out.shape[0]\n",
    "            out = out + np.eye(size,size,1)\n",
    "            #This is where we can make sure no edge has been counted for multiple times, or any\n",
    "            #other error if I think of it later\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    #We made an oopsie\n",
    "                    if out[i,j] > 1:\n",
    "                        out[i,j] = 1\n",
    "\n",
    "            adjmats.append(out)\n",
    "            if file == \"0f9e4dd3454f98f77bfa02cc37dd2f6de08f3d69.txt\":\n",
    "                print(\"blep\")\n",
    "            np.savetxt(\"../sum_data/opinosis_graphs/\" + file.replace('.txt','') + \".csv\", out, delimiter=\",\")\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        #max_size = max(max_size, len(nfile))\n",
    "        \n",
    "    return(adjmats)\n",
    "%%capture output\n",
    "computeadjmatrices(\"../sum_data/final/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blep\n"
     ]
    }
   ],
   "source": [
    "print(\"blep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this dataset, the maximum adj size is 575. We can construct the GNN with this in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to determine our vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/louis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 312136/312136 [47:51<00:00, 61.42it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769712\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "  \n",
    "set(stopwords.words('english'))\n",
    "#Go through all of the files and determine the vocabulary\n",
    "def computeunq(path):\n",
    "    words = []\n",
    "    files = os.listdir(path)\n",
    "    for file in tqdm.tqdm(files):\n",
    "        nfile = load(file)\n",
    "        if not nfile:\n",
    "            continue\n",
    "        try:\n",
    "            nfile = narr_fix_punc(fixdoubleindent(nfile))\n",
    "        \n",
    "            for elem in nfile:\n",
    "                words.append(word_tokenize(elem))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    flat_list = [item for sublist in words for item in sublist]\n",
    "    words = list(set(flat_list))\n",
    "    return(words)\n",
    "\n",
    "#unq_words = computeunq(\"pre/opinosis\")\n",
    "#print(unq_words)\n",
    "\n",
    "#print(len(unq_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import multiprocessing\n",
    "\n",
    "pool = multiprocessing.Pool()\n",
    "flat_list = list(map(lambda word: word.lower(), unq_words))\n",
    "unq_words = list(set(flat_list))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767963\n",
      "moodboards\n"
     ]
    }
   ],
   "source": [
    "#print(len(unq_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct W2V embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 97503/312136 [15:16<1:00:02, 59.58it/s]"
     ]
    }
   ],
   "source": [
    "#Loads one giant string of all of the text\n",
    "def directloadtext(path):\n",
    "    string = ''\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    for file in files:\n",
    "        nfile = load(file)\n",
    "        if not nfile:\n",
    "            continue\n",
    "        try:\n",
    "            nfile = narr_fix_punc(fixdoubleindent(nfile))\n",
    "        \n",
    "            for elem in nfile:\n",
    "                string = string + \"\\n\" + elem\n",
    "        except:\n",
    "            continue\n",
    "    return string\n",
    "\n",
    "\n",
    "def loadDataW2V(path):\n",
    "    punc = [\".\", \"!\", \"?\"]\n",
    "    string = []\n",
    "    curstring = []\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    for file in tqdm.tqdm(files):\n",
    "        curstring = dirload(file,direct=path)\n",
    "        if curstring == None:\n",
    "            continue\n",
    "        try:\n",
    "            curstring = [fix_punc2(curstring)]\n",
    "            #Split on all of the different punctuation marks\n",
    "            for elem in punc:\n",
    "                for i in range(len(curstring)):\n",
    "                    curstring[i] = curstring[i].split(elem)\n",
    "                curstring = flatten(curstring)\n",
    "                for i in range(len(curstring)):\n",
    "                    curstring[i] = curstring[i].lower()\n",
    "            for i in range(len(curstring)):\n",
    "                curstring[i] = curstring[i].split()\n",
    "                try:\n",
    "                    for j in range(len(curstring[i])):\n",
    "                        curstring[i][j] = curstring[i][j].split('\\'')\n",
    "                except:\n",
    "                    curstring[i] = curstring[i].split('\\'')\n",
    "        except:\n",
    "            continue\n",
    "        string += curstring\n",
    "    return(string)\n",
    "    \n",
    "#text = directloadtext(\"pre/opinosis/\")\n",
    "\n",
    "total_text = loadDataW2V(\"pre/opinosis/\") +  loadDataW2V(\"sum/opinosis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings = FastText(total_text, size=128, window=5, min_count=5,iter=20)\n",
    "wv = model_embeddings.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "wv.save_word2vec_format(\"../sum_data/w2v_embeddings.kv\", binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the *_vocab.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(inp):\n",
    "    counts = dict()\n",
    "    tokens = anarr_fix_punc(flatten(inp))\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in counts:ok\n",
    "            counts[token] += 1\n",
    "        else:\n",
    "            counts[token] = 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "def Reverse(tuples): \n",
    "    new_tup = tuples[::-1] \n",
    "    return new_tup \n",
    "\n",
    "count = word_count(total_text)\n",
    "sorted_count = Reverse(sorted(count.items(), key=operator.itemgetter(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dump json to file\n",
    "with open(\"../sum_data/opinosis_vocab.json\", 'w') as f:\n",
    "    json.dump(sorted_count, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the *_user.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e6caca081346d78694f2667b321f4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "def loadgold(path):\n",
    "    gold_arr = []\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        data = load(file, path)\n",
    "        if not data:\n",
    "            continue\n",
    "        string = word_tokenize(data[0])        \n",
    "        gold_arr.append(string)\n",
    "    return(gold_arr)\n",
    "\n",
    "    \n",
    "def consJSON_topics(path):\n",
    "    files = os.listdir(path)\n",
    "    i = 0\n",
    "    #Which article are we gonna try to produce the summary from\n",
    "    article_to_check = 0\n",
    "    for file in tqdm_notebook(files):\n",
    "        topic_name = file.replace('.txt', '')\n",
    "        \n",
    "        data = load(file, path)\n",
    "        if not data:\n",
    "            continue\n",
    "        string = word_tokenize(data[0])      \n",
    "        for j in range(len(string)):\n",
    "            string[j] = string[j].split('-')\n",
    "        gold_string = flatten(string)\n",
    "        \n",
    "        #What we will be saving\n",
    "        data={}\n",
    "\n",
    "\n",
    "        nfile = load(file)\n",
    "        if not nfile:\n",
    "            continue\n",
    "        try:\n",
    "            nfile = fixdoubleindent(nfile)\n",
    "\n",
    "            paragraphs = lasttofirst_str(nfile)\n",
    "            '''\n",
    "            with open(\"PGMMR/opinosis/\" + topic_name + \".txt\", 'w') as f:\n",
    "                for article in paragraphs:\n",
    "                    for elem in range(len(article)):\n",
    "                        if article[elem] in ['.', '?', '!','\\'']:\n",
    "                            article[elem] = article[elem] + \"\\n\"\n",
    "                    string = \" \".join(article)\n",
    "                    string = string.replace(\" .\\n\", \".\\n\")\n",
    "                    string = string.replace(\" ?\\n\", \"?\\n\")\n",
    "                    string = string.replace(\" !\\n\", \"!\\n\")\n",
    "                    string = string.replace(\" ,\", \",\")\n",
    "                    string = string.replace(\" \\'\", \"\\'\")\n",
    "                    string = string.replace(\"\\n \", \"\\n\")\n",
    "                    f.write(string + \"\\n\")\n",
    "\n",
    "            '''\n",
    "            #print(paragraphs)\n",
    "            sent_end = 0\n",
    "            for j in range(len(paragraphs[article_to_check])):\n",
    "                string = paragraphs[article_to_check][j]\n",
    "                if string in {\".\",\"?\",\"!\",\".\\n\",\"?\\n\",\"!\\n\"}:\n",
    "                    sent_end = j\n",
    "                    break\n",
    "\n",
    "            isCnnDM = True\n",
    "            try:\n",
    "                #Is the name in hexidecimal\n",
    "                int(topic_name, 16)\n",
    "            except:\n",
    "                isCnnDM = False\n",
    "            data['id'] = i\n",
    "            data['name'] = topic_name\n",
    "            if not isCnnDM:\n",
    "                data['gold'] = gold_string\n",
    "                data['articles'] = paragraphs\n",
    "            else:\n",
    "                data['gold'] = gold_string\n",
    "                data['articles'] = [flatten(paragraphs)]\n",
    "            with open(\"../sum_data/final/\"+ topic_name + \".json\", 'w') as f:\n",
    "                json.dump(data,f)\n",
    "\n",
    "\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        i+=1\n",
    "consJSON_topics('sum/opinosis/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175190045450373850465899563700925111600060526459"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 85577/312136 [02:20<05:19, 708.10it/s]"
     ]
    }
   ],
   "source": [
    "int(\"1eafcb17de379548083ff29304ddfa97e5836f7b\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is very poorly optimized! Be warned\n",
    "\n",
    "\n",
    "\n",
    "def constructBertEmbeddings(path, ftype = \"pre\"):\n",
    "    punc = [\".\", \"!\", \"?\"]\n",
    "    ctx = mx.gpu(0)\n",
    "    bert = BertEmbedding(ctx=ctx)    \n",
    "    string = []\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    for file in files:\n",
    "        string = dirload(file,direct=path)\n",
    "        if string == None:\n",
    "            continue\n",
    "        string = [fix_punc2(dirload(file,direct=path))]\n",
    "        #Split on all of the different punctuation marks\n",
    "        for elem in punc:\n",
    "            for i in range(len(string)):\n",
    "                string[i] = string[i].split(elem)\n",
    "            string = flatten(string)\n",
    "        result = bert(string)\n",
    "        npresult = np.array(result)\n",
    "        with open(\"../sum_data/bert_embeddings/\" + ftype + \"/\" + file,\"wb\") as file:\n",
    "            pickle.dump(npresult, file)\n",
    "            \n",
    "#constructBertEmbeddings(\"pre/opinosis/\")\n",
    "#constructBertEmbeddings(\"sum/opinosis/\", \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(npresult[3][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr = pickle.load(open(\"../sum_data/embeddings/sum/battery-life_amazon_kindle.txt\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(arr[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
